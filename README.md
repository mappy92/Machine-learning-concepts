<h3>Machine learning concepts  : </h3>
<ol>
  <li><b>Linear Regression: </b> A statistical method that models the relationship between a dependent variable and one or more independent variables by fitting a linear     equation to the observed data.</li>
  <li><b>Multivariate Regression:</b> An extension of linear regression that involves multiple dependent variables being predicted by a set of independent variables simultaneously.</li>
  <li><b>Gradient Descent and Cost Function: </b>Gradient descent is an optimization algorithm used to minimize the cost function, which measures the difference between predicted and actual values in a model, by iteratively adjusting the model's parameters.</li>
  <li><b>Save and Load a Model: </b>The process of saving a trained machine learning model to a file and later loading it for future use without retraining, ensuring consistency and efficiency.</li>
  <li><b>Dummy Variable and One-Hot Encoder:</b> Techniques used to convert categorical variables into numerical form, where dummy variables represent categories with binary values, and one-hot encoding creates a binary column for each category.</li>
  <li><b>Split Data - Train and Test:</b> The practice of dividing a dataset into training and testing subsets, where the training set is used to build the model and the test set is used to evaluate its performance.</li>
  <li><b>Logistic Regression - Binary: </b>A classification algorithm used to predict a binary outcome (two possible classes) based on one or more independent variables.</li>
  <li><b>Logistic Regression - Multiclass:</b> An extension of logistic regression that handles multiple classes by applying techniques like one-vs-rest or softmax to predict more than two categories.</li>
  <li><b>Decision Tree: </b>A non-linear model that splits data into branches based on feature values, forming a tree-like structure to make decisions or predictions.</li>
  <li><b>Support Vector Machine (SVM): </b>A classification algorithm that finds the optimal hyperplane to separate different classes by maximizing the margin between 
    them.</li>
  <li><b>Random Forest: </b>An ensemble learning method that combines multiple decision trees to improve accuracy and reduce overfitting by averaging their predictions.</li>
  <li><b>K-Fold Cross Validation: </b>A technique for assessing the performance of a model by dividing the dataset into K subsets, training the model on K-1 subsets, and validating it on the remaining subset, rotating through all subsets.</li>
  <li><b>K-Means Clustering: </b>An unsupervised learning algorithm that partitions data into K distinct clusters by minimizing the distance between data points and their respective cluster centroids.</li>
  <li><b>Naive Bayes: </b>A classification algorithm based on Bayes' theorem, assuming independence between features, commonly used for text classification tasks like spam detection.</li>
  <li><b>Hyperparameter Tuning:</b> The process of optimizing the parameters that control the learning process of a model to improve its performance on unseen data.</li>
  <li><b>K-Nearest Neighbors (KNN):</b> A simple, instance-based learning algorithm that classifies data points based on the majority class among the K-nearest neighbors in the feature space.</li>
  <li><b>Principal Component Analysis (PCA) - Dimension Reduction: </b>A technique for reducing the dimensionality of data by transforming it into a set of orthogonal components that capture the most variance.</li>
  <li><b>Bagging Technique: </b>A method that involves training multiple models on different subsets of the training data and combining their predictions to improve accuracy and reduce variance.</li>
</ol>










